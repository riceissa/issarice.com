---
title: Reasons for altruism
author: Issa Rice
created: 2018-03-06
date: 2018-03-06
# documentkind:
status: notes
# belief:
---

It occurs to me that there are many reasons (including many weird reasons) for altruism, so I'll try writing some of these down.

- The typical reason seems to be that you value the welfare of others and the discount factor you apply for them (for not being you) isn't too low. So when making decisions, you take your own welfare into account, plus the aggregate (discounted) welfare of others.
- Even if you discount the welfare of others to be arbitrarily low, as long as you have moral uncertainty and use something like the ["expected moral value"](http://users.ox.ac.uk/~mert2255/papers/mu-about-pe.pdf "Hilary Graves; Toby Ord. “Moral uncertainty about population axiology”.") approach, and there are sufficiently many other beings you could help, [swamping](https://causeprioritization.org/Swamping_(population_ethics)) could occur. This is because worldviews that subscribe to altruism think there is "more at stake" in the world.
- You could use a [Moral Parliament](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html "Nick Bostrom (January 1, 2009). “Moral uncertainty – towards a solution?” Overcoming Bias.") framework to make a ["veil of ignorance" or FDT-like deal](https://causeprioritization.org/Veil_of_ignorance_and_functional_decision_theory) in which in worlds where you can have a lot of impact, you act essentially like an altruist, whereas in worlds where you can't have much impact you act basically like a selfish person. And it appears that in this world we can have a lot of impact.[^bottleneck]
- If one of the Big World theories is correct, there could be many distant copies of you across the universe or multiverse. If you care about these copies, you might take actions to help them that end up helping many others. So here you don't necessarily care about non-copies but you end up helping them anyway.
- A variant of the previous one is that even if the World is small, you might care about the pseudo-copies of you that other people create as they model your behavior. See [this post](http://lesswrong.com/lw/1ay/is_cryonics_necessary_writing_yourself_into_the/ "gworley (June 23, 2010). “Is cryonics necessary?: Writing yourself into the future”. LessWrong.") for a similar idea.
- You have many shared memories/experiences/decision algorithms/genes as many other people. If what you value about yourself is one of these things, you might care about the instances of these things even when they are in other people. Similarly you could take a ["pan-everythingism about everything"](http://files.openphilanthropy.org/files/Conversations/Brian_Tomasik_10-06-16_(public).pdf) approach, where many other things in the world *are* you to varying degrees. And presumably other humans are more like you than, say, rocks.
- Other reasons for cooperation (e.g. in Newcomb-like situations) might produce altruistic actions.

[^bottleneck]: [“Wei\_Dai comments on Max Tegmark on our place in history: "We're Not Insignificant After All"”](http://lesswrong.com/lw/1li/max_tegmark_on_our_place_in_history_were_not/1eer). LessWrong. Retrieved March 7, 2018.

    > What strikes me about our current situation is not only are we at an extremely influential point in the history of the universe, but how few people realize this. It ought to give the few people in the know enormous power (relative to just about anyone else who has existed or will exist) to affect the future, but, even among those who do realize that we're at a bottleneck, few try to shape the future in any substantial way, to nudge it one way or another. Instead, they just go about their "normal" lives, and continue to spend their money on the standard status symbols and consumer goods.
