## a counter-piracy measure for non-fiction

Non-fiction works usually contain a bunch of facts and figures that can be
detected and automatically substituted to form different versions. In other
words, a non-fiction work can be used to create a template/schema from which a
great many other "versions" of the work, containing incorrect figures, can be
spawned off. If a publisher then distributes these incorrect versions to
various pirating sites, the consumer of the pirated version usually has no way
to tell if the version they have is genuine or incorrect. Perhaps with more
advanced AI systems it will be possible to implement various "checking"
routines to see if the incorrect figures still *locally* make sense (e.g.
checking if the tampered percentages still add up to 100%, to see if there are
no direct contradictions in the work, and so forth). The publisher can then
discard versions that fail to pass such a battery of tests. In the end, a user
of the pirated version can run his own battery of tests but without locating
any contradictions.

The end result is that users of pirated versions must obtain pirated copies
from "trusted" sources or else purchase genuine versions.

For fictional work, such a procedure is more difficult to implement as it is
the story or entertainment value that matters, which can be judged by the
consumer regardless of whether a version of the work is "incorrect". ("If it's
entertaining, who cares if it's fake?")

John Wentworth has a similar idea for improving the academic peer review process:
https://www.lesswrong.com/posts/Cy8p6NMJ9kELYpR8Z/what-are-some-civilizational-sanity-interventions?commentId=6L6ZRshadaJoMgDe7
In both cases, the idea is to flood some space with low quality but hard-to-distinguish-from-the-real-thing objects so that people pause to try to distinguish the real thing from the fake things.
