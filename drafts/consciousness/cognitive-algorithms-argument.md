# Cognitive algorithms argument

> Strong agreement about not overgeneralizing. It does appear, however, that
> libertarianism about free well, non-physicalism about the mind, and a number
> of sorts of moral realism form a cluster, sharing the feature of reifying
> certain concepts in our cognitive algorithms even when they can be 'explained
> away.' Maybe we can discuss this tomorrow night.

http://lesswrong.com/lw/1oj/complexity_of_value_complexity_of_outcome/1jfx

> Michael Vassar has observed that conventional philosophers seem to be
> *spectacularly* bad at understanding that their intuitions are generated by
> cognitive algorithms. This may be why works of serious reductionism get
> written by Artificial Intelligence people instead of conventional
> philosophers.

https://wiki.lesswrong.com/wiki/How_an_algorithm_feels

http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside/

> It seems that you get similar questions as a natural outgrowth of simple
> computational models of thought. E.g. if one performs Solomonoff induction on
> the stream of camera inputs to a robot, what kind of short programs will
> dominate the probability distribution over the next input? Not just programs
> that simulate the physics of our universe: one would also need additional
> code to "read off" the part of the simulated universe that corresponded to
> the camera inputs. That additional code looks like epiphenomenal mind-stuff.
> Using this framework you can pose questions like "if the camera is expected
> to be rebuilt using different but functionally equivalent materials, will his
> change the inputs Solomonoff induction predicts?" or "if the camera is about
> to be duplicated, which copy's inputs will be predicted by Solomonoff
> induction?"
>
> If we go beyond Solomonoff induction to allow actions, then you get questions
> that map pretty well to debates about "free will."

http://lesswrong.com/lw/f1u/causal_reference/7o7j

Keith Frankish's "Illusionism as a Theory of Consciousness".
