'''Michael Vassar''' (born February 4, 1979) is an American futurist, activist, and entrepreneur. His career has focused on the prevention of [[wikipedia:Global catastrophic risk|global catastrophic risk]] from [[wikipedia:Emerging technologies|emerging technology]].

==Quotes==

* "Imagine there is a set of skills," [Michael Vassar] said. "There is a myth that they are possessed by the whole population, and there is a cynical myth that they're possessed by 10 percent of the population. They've actually been wiped out in all but about one person in three thousand." It is important, Vassar said, that his people, "the fragments of the world," lead the way during "the fairly predictable, fairly total cultural transition that will predictably take place between 2020 and 2035 or so." We pulled up outside the Rose Garden Inn. He continued: "You have these weird phenomena like Occupy where people are protesting with no goals, no theory of how the world is, around which they can structure a protest. Basically this incredibly, weirdly, thoroughly disempowered group of people will have to inherit the power of the world anyway, because sooner or later everyone older is going to be too old and too technologically obsolete and too bankrupt. The old institutions may largely break down or they may be handed over, but either way they can't just freeze. These people are going to be in charge, and it would be helpful if they, as they come into their own, crystallize an identity that contains certain cultural strengths like argument and reason."
** Quoted in Sam Frank, [https://harpers.org/archive/2015/01/come-with-us-if-you-want-to-live/5/ "Come With Us If You Want to Live: Among the apocalyptic libertarians of Silicon Valley"], January 2015

* I know it's tedious, but you aren't SL4 until you appreciate the depth of human irrationality. Yes rationality is there, but at a level that just barely shows up when measured with precise instruments. This is important with respect to non-human intelligences, because super-rationality, almost as much as superintelligence, is potentially overwhelming and because intelligence bootstraping moves a system towards rationality. Appreciate how far humans are from rational and you appreciate how utterly transformed, and essentially recreated, they would be by haphazard bootstrapping. Appreciate how formidable rationality is and you see why a highly rational infrahuman GAI would still be a massive existential threat.
** [http://sl4.org/archive/0602/14276.html "Another reminder of irrationality"], February 2006

* Whenever I meet a person with a high school diploma and an IQ > 150 I think "not ambitious enough".
** In a comment on [http://econlog.econlib.org/archives/2007/06/tyler_may_not_a.html#26346 "Tyler May Not Agree With Me On Education, But His Inner Economist Does"], June 2007

==Quotes about Vassar==

* Michael Arc once gave me and Ofer Grossman quests. My quest was to write a Go AI capable of defeating world champions, and Ofer's was to purchase a metric ton of corn. One of these quests has been accomplished (though certainly not by us), but the other one still stands...
** Qiaochu Yuan, in a [https://www.facebook.com/qiaochu/posts/10154105311135811 Facebook post], January 2017

* I am personally convinced that the LW Sequences / AI to Zombies gave me something, and gave something similar to others I know, and that '''hanging out in person with Eliezer Yudkowsky, Michael Vassar, Carl Shulman, Nick Bostrom, and others gave me more of that same thing'''; a "same thing" that included e.g. actually trying to figure it out, making beliefs pay rent in anticipated experience; using arithmetic to entangle different pieces of my beliefs; and so on.
** Anna Salamon, [http://lesswrong.com/lw/n4e/why_cfars_mission/ "Why CFAR's Mission?"], January 2016

* You listen to Michael Vassar. You don't remember traveling to this party or sitting on this beanbag. You don't remember when he began to speak. He is still speaking. '''He sounds like madness and glory given lisping poetry, and you want to obey.'''
** Alicorn, [http://luminousalicorn.tumblr.com/post/115832211805/rationalism-gothic "Rationalism Gothic"], April 2015

* Vassar, before I came to Berkeley, someone warned me "'''Vassar is kind of crazy and it's impossible to have a normal conversation with him'''". As a result, I spent several months avoiding you. Then I finally got to meet you and I realized I had made a huge mistake. I mean, you are crazy, and it is is impossible to have a normal conversation with you. But normal conversation is incredibly over-rated compared to whatever the heck you call the thing that interaction with you involves. I regret that we didn't get more of a chance to talk about stuff and I hope to solve that sometime in the future.
** Scott Alexander, [http://slatestarcodex.com/2013/05/24/going-from-california-with-an-aching-in-my-heart/ "Going From California With An Aching In My Heart."], May 2013

* '''One of the most brilliant people I have ever met is Michael Vassar.''' I first ran into him at the Transvision 2003 Conference, just over three years ago. I was sitting next to my sister in a gorgeous auditorium at Yale, and we were watching the opening session of the conference&nbsp;&ndash; a debate between Gregory Stock, a transhumanist, and George Annas, who objected to genetic modifications to the human body. At the end of the talk, during the Q&A session, Vassar stood up and asked a question, which was more like a rebuttal, to George Annas&nbsp;&ndash; he pointed out that animal experiments have shown that artificial selection can rapidly produce results, like stronger bodies and longer lifespans, so what is the point of restricting genetic modifications, when they can already be achieved so simply? (Paraphrasing.)
** Michael Anissimov, [https://web.archive.org/web/20090119095012/http://acceleratingfuture.com/michael/blog/2006/08/michael-vassars-papers/ "Michael Vassar's Papers"], August 2006

==External links==

* [http://lesswrong.com/user/MichaelVassar/overview/ LessWrong profile]

{{Wikipedia}}

{{DEFAULTSORT:Vassar, Michael}}
