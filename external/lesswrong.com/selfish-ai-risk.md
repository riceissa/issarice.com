Comment to be made on https://www.greaterwrong.com/posts/HnC29723hm6kJT7KP/taking-ai-risk-seriously-thoughts-by-critch

https://forum.effectivealtruism.org/posts/7DhEnxBqP62jHmsAx/taking-ai-risk-seriously-thoughts-by-andrew-critch#CgS7hKqzsfPrBRpA8

> Even if you’re not interested in orienting your life around helping with x-risk – if you just want to not be _blindsided_ by radical changes that may be coming
>
> [...]
>
> We don't know exactly what will happen, but I expect serious changes of some sort over the next 10 years. Even if you aren't committing to saving the world, I think it's in your interest just to understand what is happening, so in a decade or two you aren't completely lost.
>
> And even 'understanding the situation' is complicated enough that I think you need to be able to quit your day-job and focus full-time, in order to get oriented.

Raymond, do you or Andrew Critch have any concrete possibilities in mind for what "orienting one's life"/"understanding the situation" might look like from a non-altruistic perspective? I'm interested in hearing concrete ideas for what one _might_ do; the only suggestions I can recall seeing so far were mentioned in the [80,000 Hours podcast episode with Paul Christiano](https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/), to save money and invest in certain companies. Is this the sort of thing you had in mind?

The way I am imagining it, a person thinking about this from a non-altruistic perspective would then think about the problem for several years and would narrow this list down (or add new things to it) and act on some subset of them (e.g. maybe they would think about which companies to invest in and decide how much money to save, but to not implement some other idea). Is this an accurate understanding of your view?
