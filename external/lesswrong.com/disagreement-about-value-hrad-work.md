# Plausible cases for HRAD work, and locating the crux in the "realism about rationality" debate

each world is like a positive case for doing HRAD work, and the objection to it in that case

## Clarifying some terms

terms to clarify:

- precise vs imprecise theory
- "2+ levels"
- building agents from the ground up
- predicting what ML systems do

## World 1

### Case for HRAD

relatively-imprecise theories of agents as final product is OK (this is like what eliezer seems to be saying in some places)

this is like deconfusion maximalism: the whole point is to become less confused, and it doesn't matter too much whether the final theory is precise or imprecise.

### Why I think we might be in this world

e.g. "We’re working on decision theory because there’s a cluster of confusing issues here (e.g., counterfactuals, updatelessness, coordination) that represent a lot of holes or anomalies in our current best understanding of what high-quality reasoning is and how it works." and phrases like "developing an understanding of roughly what counterfactuals are and how they work" and "very roughly how/why it works" https://www.greaterwrong.com/posts/uKbxi2EJ3KBNRDGpL/comment-on-decision-theory -- this post then doesn't really specify whether or not the final output is expected to be precise. (The analogy with probability theory and rockets gestures at precise theories, but the post doesn't come out and say it.)

"I don't think there's a true rationality out there in the world, or a true decision theory out there in the world, or even a true notion of intelligence out there in the world. I work on agent foundations because there's *still something I'm confused about* even after that, and furthermore, AI safety work seems fairly hopeless while still so radically confused about the-phenomena-which-we-use-intelligence-and-rationality-and-agency-and-decision-theory-to-describe." https://www.lesswrong.com/posts/suxvE2ddnYMPJN9HD/realism-about-rationality?commentId=XEbNPAyvjpTcGufLm

### The crux

rejected by (1)

## World 2

### Case for HRAD

HRAD work can produce precise theories of predicting ML systems, which can scale 2+ levels. This theory can then be used to detect/fix problems and ensure the safety of AGI systems.

### Why I think we might be in this world

### The crux

rejected by (3)

## World 3

### Case for HRAD

HRAD work can produce precise theories *of building agents from the ground up*, but not precise theories about how to predict ML systems or anything like that.
so the case is that doing the former will help with the latter in broad strokes, even if not ?

### Why I think we might be in this world

### The crux

rejected by (4), or by rejecting the existence of the precise theory using (3).
i think daniel dewey might be here..., and rejects it via 4, not 3.

## World 4

### Case for HRAD

low standards/success criteria world

### Why I think we might be in this world

### The crux





rohin's three-step argument: ok, let's see if it applies to shannon's work in chess.
1. this is a general statement
2. doesn't apply to chess?
3. so is shannon's theory imprecise? i would guess it's precise. so this might be the disanalogy with what miri is attempting.

# Is the disagreement about the value of HRAD work about success criteria or about ability to achieve agreed-upon aims?

I've been thinking about the debate about the value of MIRI's [highly reliable agent designs](https://intelligence.org/files/TechnicalAgenda.pdf) work (which includes work on decision theory), for example the discussions in [Realism about rationality](https://www.lesswrong.com/posts/suxvE2ddnYMPJN9HD/realism-about-rationality) and Daniel Dewey's [My current thoughts on MIRI's "highly reliable agent design" work](https://forum.effectivealtruism.org/posts/SEL9PW8jozrvLnkb4/my-current-thoughts-on-miri-s-highly-reliable-agent-design).

Something I feel confused about is the extent to which the disagreement is about success criteria vs MIRI's ability to achieve agreed-upon aims. That's a very abstract way of phrasing it, so I want to be more concrete about what I mean.

(Throughout this post I will attribute beliefs to both MIRI and HRAD skeptics. Obviously both groups are heterogeneous and not everyone within a given group will have the same opinion, etc. I don't think it makes a big difference but if you think it does, feel free to suggest some better wording.)

1. "Disagreement is about success criteria" framing: MIRI has one set of success criteria for HRAD work, which is something like "conceptually clarifies the alignment problem; does a thing analogous to the work of Shannon, Turing, Pearl; helps people avoid mistakes analogous to the use of null-terminated strings in C" (let's call these criteria A). Skeptics of HRAD work have another set of criteria for success, which is something like "axiomatically blah blah" (let's call these criteria B). Skeptics agree that criteria A are achievable, but believe that achieving criteria A is insufficient for actually being useful for aligning an AGI. Instead, to be helpful for aligning an AGI, HRAD work must achieve criteria B, and either MIRI isn't even working toward criteria B or else HRAD work won't accomplish criteria B, so HRAD work won't be useful for aligning an AGI.
2. "Disagreement is about MIRI's ability to achieve agreed-upon aims" framing: Both MIRI and skeptics agree on what counts as success for HRAD work, i.e. there is some single success criterion that both sides agree on, which is some mixture of the criteria I listed above. The disagreement is instead about whether MIRI can achieve those goals.


you need to achieve this more difficult thing B; only doing A is insufficient. -- disagreement about success criteria

vs

both sides agree that X is the success criteria.
but then skeptics are like "well i don't think you can do X"
